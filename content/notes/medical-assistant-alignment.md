+++ 
draft = true

title = "A Biomedical Assistant as a Laboratory for Alignment Research"
description = ""

tags = []
categories = []
series = []

date = 2023-05-26T20:34:23-04:00
hideReadingTime = false
katex = false
+++

I've had the pleasure recently of working on Med-PaLM and Med-PaLM 2 with an extremely talented group of people. I wanted to jot some notes down here about the personal motivation that got me working in the medical setting, and why I think work in this direction could be important to ensuring AI is safe and beneficial.

Safety is highly, highly critical in this domain, both for shorter-term clinical workflows and longer-term progress towards increasingly general AI that can progress science. This is crucial for the team and aligns well with Anthropic’s mission

    
This setting might be a better “laboratory for alignment research” than the general chatbot setting. For example, as Med-PaLM 2 has reached expert-level at medical question answering, scalable oversight is an immediate and pressing technical issue for integrating human feedback into these models. It’s also more obvious here that some of the fundamental issues with LLMs need to be solved here (vs. a general chatbot) in order to realize full real-world impact (product-oriented alignment for ChatGPT / Claude may not require solving these problems).


* Can't do alignment research in a vacuum
* 